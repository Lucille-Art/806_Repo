{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(url1,url2):\n",
    "    \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests as r\n",
    "    import numpy as np\n",
    "    import time\n",
    "\n",
    "    resp_indeed = r.get(url1)\n",
    "    resp_pe = r.get(url2)\n",
    "    content_type_indeed = resp_indeed.headers['Content-Type'].lower()\n",
    "    content_type_pe = resp_pe.headers['Content-Type'].lower()\n",
    "    \n",
    "    if (resp_indeed.status_code==200 and content_type_indeed is not None and content_type_indeed.find('html')>-1) & (resp_pe.status_code==200 and content_type_pe is not None and content_type_pe.find('html')>-1):\n",
    "        print('We can start downloading the datas.\\n\\n')\n",
    "        \n",
    "    else:\n",
    "        print('There is an issue accessing the server.')\n",
    "        \n",
    "    return get_datas_indeed(url1,url2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datas_indeed(url1,url2):\n",
    "    \n",
    "    step=0\n",
    "    i=10\n",
    "    \n",
    "    locations_indeed =[]\n",
    "    companies_indeed =[]\n",
    "    jobs_indeed =[]\n",
    "    dates_indeed =[]\n",
    "    rates_indeed=[]\n",
    "    rates=[]\n",
    "    \n",
    "    while i>=0:\n",
    "        url1=f'https://www.indeed.fr/jobs?q=data+analyst&start={i}'\n",
    "        resp_indeed=r.get(url1)\n",
    "        soup_indeed = BeautifulSoup(resp_indeed.content)\n",
    "\n",
    "        if step%5==0:\n",
    "            time.sleep(2.4)\n",
    "            print('[Indeed] Page',step,'is done.')\n",
    "\n",
    "        locations_indeed += [i.text for j in [i.select('.location') for i in soup_indeed.select('div.jobsearch-SerpJobCard')] for i in j]       \n",
    "        companies_indeed += [i.text.strip('\\n').title() for i in soup_indeed.select('span.company')]\n",
    "        jobs_indeed += [i.text.lower().strip('\\n (h/f) h/f').capitalize() for i in soup_indeed.select('h2')]\n",
    "        dates_indeed += [i.text.lower().lstrip('il y a plus de').rstrip('jours ') for i in soup_indeed.select('span.date')]\n",
    "        for j in [i.select('.ratingsContent') for i in soup_indeed.select('div.jobsearch-SerpJobCard')]:\n",
    "            if j==[]:\n",
    "                rates.append('Not specified')\n",
    "            else:\n",
    "                rates.append([k.text.strip('\\n') for k in j])\n",
    "\n",
    "        step+=1\n",
    "        i+=10\n",
    "\n",
    "        if i>=660:\n",
    "            print('Indeed is done.\\n')\n",
    "\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    dict_indeed = {'Job': jobs_indeed,\n",
    "        'Company': companies_indeed,\n",
    "        'Location': locations_indeed,\n",
    "        'Publication date': dates_indeed,\n",
    "        'Company\\'s rate':rates}\n",
    "\n",
    "            \n",
    "    return get_datas_pe(dict_indeed, url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datas_pe(dict_indeed,url2):\n",
    "    \n",
    "    step=0\n",
    "    i=0\n",
    "    \n",
    "    locations_pe =[]\n",
    "    jobs_pe =[]\n",
    "    dates_pe = []\n",
    "\n",
    "    while i>=0:\n",
    "        url2=f'https://candidat.pole-emploi.fr/offres/recherche.rechercheoffre:afficherplusderesultats/{i}-{i+9}?motsCles=data+analyst&offresPartenaires=true&range={i-10}-{i-1}&rayon=10&tri=0'\n",
    "        resp_pe=r.get(url2)\n",
    "        soup_pe = BeautifulSoup(resp_pe.content)\n",
    "\n",
    "        if step%5==0:\n",
    "            time.sleep(2.4)\n",
    "            print('[Pôle Emploi] Page',step,'is done.')\n",
    "\n",
    "        locations_pe += [i.text.strip('\\n').title() for i in soup_pe.select('p.subtext')]\n",
    "        jobs_pe += [i.text.lower().strip('\\n (h/f) h/f').capitalize() for i in soup_pe.select('h2.t4')]\n",
    "        dates_pe += [i.text.lower().strip('\\n publié il y a jours de') for i in soup_pe.select('p.date')]\n",
    "      \n",
    "        step +=1\n",
    "        i+=10\n",
    "\n",
    "        if i>=185:\n",
    "            print('Pôle Emploi is done.')\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    dict_pe = {'Job': jobs_pe,\n",
    "             'Location': locations_pe,\n",
    "              'Publication date':dates_pe}\n",
    "    \n",
    "    return into_dataframe(dict_indeed,dict_pe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def into_dataframe(dict_indeed, dict_pe):\n",
    "    \n",
    "    df1=pd.DataFrame(dict_indeed)\n",
    "    df2=pd.DataFrame(dict_pe)\n",
    "    df = pd.concat([df1,df2])\n",
    "    \n",
    "    return cleaning_df(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_df(df):\n",
    "    \n",
    "    df['Company'] = df['Company'].fillna('Not specified')\n",
    "    df['Company\\'s rate'] = df['Company\\'s rate'].fillna('Not specified')\n",
    "    df['Publication date'] = df['Publication date'].fillna(100)\n",
    "    df.loc[df['Publication date']=='d\\'h', 'Publication date']=1\n",
    "    df.loc[df['Publication date']=='jourd\\'hui', 'Publication date']=0\n",
    "    df.loc[df['Publication date']=='bliée à l\\'instant', 'Publication date']=0\n",
    "    df.loc[df['Publication date']=='Not specified', 'Publication date']=100\n",
    "    df.loc[df['Publication date']=='h', 'Publication date']=1\n",
    "    df.loc[df['Publication date']=='\\'h', 'Publication date']=1\n",
    "    df.loc[df['Publication date']=='30 jours\\n\\noffre avec peu de candidat', 'Publication date']=30\n",
    "    df['Publication date'].unique()\n",
    "    df['Publication date']=df['Publication date'].astype(int)\n",
    "    \n",
    "    df=df.sort_values(by='Publication date')\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    display(df)\n",
    "    \n",
    "    return saving_df(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_df(df):\n",
    "    \n",
    "    save_df=True\n",
    "\n",
    "    while save_df:\n",
    "        save = str(input('Do you want to save the changes in the database ?:'))\n",
    "        if save=='yes':\n",
    "            df.to_csv('Jobs.csv',index=False)\n",
    "            print('Changes are saved.')\n",
    "            save_df=False\n",
    "        \n",
    "        elif save=='no':\n",
    "            print('Changes not saved.')\n",
    "            save_df=False\n",
    "            \n",
    "        else:\n",
    "            print('You must answer \"yes\" or \"no\".')\n",
    "            continue\n",
    "            \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can start downloading the datas.\n",
      "\n",
      "\n",
      "[Indeed] Page 0 is done.\n",
      "[Indeed] Page 5 is done.\n",
      "[Indeed] Page 10 is done.\n",
      "[Indeed] Page 15 is done.\n",
      "[Indeed] Page 20 is done.\n",
      "[Indeed] Page 25 is done.\n",
      "[Indeed] Page 30 is done.\n",
      "[Indeed] Page 35 is done.\n",
      "[Indeed] Page 40 is done.\n",
      "[Indeed] Page 45 is done.\n",
      "[Indeed] Page 50 is done.\n",
      "[Indeed] Page 55 is done.\n",
      "[Indeed] Page 60 is done.\n",
      "Indeed is done.\n",
      "\n",
      "[Pôle Emploi] Page 0 is done.\n",
      "[Pôle Emploi] Page 5 is done.\n",
      "[Pôle Emploi] Page 10 is done.\n",
      "[Pôle Emploi] Page 15 is done.\n",
      "Pôle Emploi is done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Publication date</th>\n",
       "      <th>Company's rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ingénieur data analyst</td>\n",
       "      <td>Segula Technologies</td>\n",
       "      <td>La Rochelle (17)</td>\n",
       "      <td>0</td>\n",
       "      <td>[3,0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ingénieur data analyst</td>\n",
       "      <td>Segula Technologies</td>\n",
       "      <td>La Rochelle (17)</td>\n",
       "      <td>0</td>\n",
       "      <td>[3,0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ingénieur data analyst</td>\n",
       "      <td>Segula Technologies</td>\n",
       "      <td>La Rochelle (17)</td>\n",
       "      <td>0</td>\n",
       "      <td>[3,0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analyste de veille marchés publics (appels d'o...</td>\n",
       "      <td>Octopusmind</td>\n",
       "      <td>Nantes (44)</td>\n",
       "      <td>0</td>\n",
       "      <td>Not specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ingénieur data analyst</td>\n",
       "      <td>Segula Technologies</td>\n",
       "      <td>La Rochelle (17)</td>\n",
       "      <td>0</td>\n",
       "      <td>[3,0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>Data manager / data analyst</td>\n",
       "      <td>Digitick</td>\n",
       "      <td>Marseille (13)</td>\n",
       "      <td>30</td>\n",
       "      <td>[4,0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>Data analyst</td>\n",
       "      <td>Ogilvy</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>30</td>\n",
       "      <td>[4,1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>Data analyst - business intelligence junior</td>\n",
       "      <td>Bca Expertise</td>\n",
       "      <td>Asnières-sur-Seine (92)</td>\n",
       "      <td>30</td>\n",
       "      <td>[3,1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>Data analyst connaissance clients</td>\n",
       "      <td>Le Groupe La Poste</td>\n",
       "      <td>Issy-les-Moulineaux (92)</td>\n",
       "      <td>30</td>\n",
       "      <td>[3,8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>Analyste quantitatif surveillance des marchés ...</td>\n",
       "      <td>Amf</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>30</td>\n",
       "      <td>[3,3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1275 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Job              Company  \\\n",
       "0                                Ingénieur data analyst  Segula Technologies   \n",
       "1                                Ingénieur data analyst  Segula Technologies   \n",
       "2                                Ingénieur data analyst  Segula Technologies   \n",
       "3     Analyste de veille marchés publics (appels d'o...          Octopusmind   \n",
       "4                                Ingénieur data analyst  Segula Technologies   \n",
       "...                                                 ...                  ...   \n",
       "1270                        Data manager / data analyst             Digitick   \n",
       "1271                                       Data analyst               Ogilvy   \n",
       "1272        Data analyst - business intelligence junior        Bca Expertise   \n",
       "1273                  Data analyst connaissance clients   Le Groupe La Poste   \n",
       "1274  Analyste quantitatif surveillance des marchés ...                  Amf   \n",
       "\n",
       "                      Location  Publication date Company's rate  \n",
       "0             La Rochelle (17)                 0          [3,0]  \n",
       "1             La Rochelle (17)                 0          [3,0]  \n",
       "2             La Rochelle (17)                 0          [3,0]  \n",
       "3                  Nantes (44)                 0  Not specified  \n",
       "4             La Rochelle (17)                 0          [3,0]  \n",
       "...                        ...               ...            ...  \n",
       "1270            Marseille (13)                30          [4,0]  \n",
       "1271                Paris (75)                30          [4,1]  \n",
       "1272   Asnières-sur-Seine (92)                30          [3,1]  \n",
       "1273  Issy-les-Moulineaux (92)                30          [3,8]  \n",
       "1274                Paris (75)                30          [3,3]  \n",
       "\n",
       "[1275 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to save the changes in the database ?:yes\n",
      "Changes are saved.\n"
     ]
    }
   ],
   "source": [
    "url1 = 'https://www.indeed.fr/jobs?q=data+analyst&start=20'\n",
    "url2 = 'https://candidat.pole-emploi.fr/offres/recherche.rechercheoffre:afficherplusderesultats/10-19?motsCles=data+analyst&offresPartenaires=true&rayon=10&tri=0'\n",
    "\n",
    "scrap(url1,url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
